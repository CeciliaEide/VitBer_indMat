{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosjekt 2: Transformermodellen for prediksjon av sekvenser\n",
    "Av: Ingrid Løvold, Oskar Farbrot og Cecilia Eide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.1\n",
    "Vi har at  \n",
    "* $d = a \\times b + c$ \n",
    "* $a, c$ er tosifrede heltall, mens $b$ er et ettsifret heltall, altså $9 ≥ b ∈ Z$ og vi kan derfor skrive de som:\\\n",
    "$a = [a_0, a_1]$\\\n",
    "$b = [b_0]$\\\n",
    "$c = [c_0, c_1]$\n",
    "\n",
    "Lengden av $d$ vil maksimalt være $3$ siffer fordi $d_{\\text{max}} = 99 \\times 9 + 99 = 990$.\n",
    "\n",
    "For å opprette et sett med trengingsdata kan vi lage en $x$ som består av sifrene i $a$, $b$ , $c$ og $d$, unntatt det siste fra $d$. Vi lar $y$ være lik \"fasiten\" vår $d$. I et datasett hvor for $a$ og $c$ har ett mer siffer enn $b$, og vi setter $r =$ antall sifre i $a$, vil dermed få treningsdata på følgende form:\n",
    "\n",
    "$x = [a_0, . . . , a_{r−1}, b_0, . . . , b_{r−2}, c_0, . . . , c_{r−1}, d_0, . . . , d_{r-1}]$   ; der $a_i, b_i, c_i ∈ {0, . . . , 9}$\n",
    "\n",
    "$y = [d_0, . . . , d_r]$\n",
    "\n",
    "Som for dette tilfellet, hvor $r = 2$, blir:\n",
    "\n",
    "$x = [a_0, a_1, b_0, c_0, c_1, d_0, d_1]$\n",
    "\n",
    "$y = [d_0, d_1, d_2]$\n",
    "\n",
    "For å illustrere et eksempel på et treningssett setter vi opp problemet $86 \\times 4 + 28 = 372$\n",
    "Vi lar $r = 2$, $a = 86, b = 4, c = 28$.\n",
    "Da får vi \n",
    "\n",
    "$x = [8, 6, 4, 2, 8, 3, 7], y = [3, 7, 2]$\n",
    "\n",
    "Modellen vil gi oss $z = [\\hat{z}_0, . . . , \\hat{z}_6] = f_θ([8, 6, 4, 2, 8, 3, 7]) $\n",
    "Og vi ønsker å finne $θ$ slik at $\\hat{y} = [\\hat{z}_4, \\hat{z}_5, \\hat{z}_6] = y = [3, 7, 2] = d$. \n",
    "\n",
    "I implementeringen blir det naturlig å behandle rekkefølgen på $y$ annerledes. Siden det siste sifferet kan regnes ut uavhengig av siffrene før, vil dette bli predikert først og rekkefølgen er reverset. I praksis vil derfor treningssettet vårt se slik ut:\\\n",
    "$x = [a_0,a_1,b_0,c_0,c_1,d_2,d_1], y = [d_2,d_1,d_0]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.2\n",
    "Når optimeringen er ferdig, hvordan kan vi bruke modellen $f_θ$ til å predikere $d$ gitt\n",
    "$a, b, c$? Vis dette med et eksempel, på samme måte som i likning (11).\n",
    "\n",
    "\n",
    "$x_0 = [8,6,4,2,8] \\quad [\\hat{z}_0,\\hat{z}_1,\\hat{z}_2,\\hat{z}_3,\\hat{z}_4,\\hat{z}_5] = f_\\theta(x_0)$\n",
    "\n",
    "$x_1 = [8,6,4,2,8,\\hat{z}_5] \\quad [\\hat{z}_0,\\hat{z}_1,\\hat{z}_2,\\hat{z}_3,\\hat{z}_4,\\hat{z}_5,\\hat{z}_6] = f_\\theta(x_1)$\n",
    "\n",
    "$x_2 = [8,6,4,2,8,\\hat{z}_5,\\hat{z}_6] \\quad [\\hat{z}_0,\\hat{z}_1,\\hat{z}_2,\\hat{z}_3,\\hat{z}_4,\\hat{z}_5,\\hat{z}_6,\\hat{z}_7] = f_\\theta(x_2)$\n",
    "\n",
    "$x_3 = [8,6,4,2,8,\\hat{z}_5,\\hat{z}_6,\\hat{z}_7] \\quad [\\hat{z}_0,\\hat{z}_1,\\hat{z}_2,\\hat{z}_3,\\hat{z}_4,\\hat{z}_5,\\hat{z}_6,\\hat{z}_7,\\hat{z}_8] = f_\\theta(x_3)$\n",
    "\n",
    "\n",
    "Vi kan sammenligne $ŷ$ med $y$ (fasit) for å se om modellen fungerer, dette gjøres i loss-funksjonen.\n",
    "\n",
    "\n",
    "$f_θ$ har som mål å minimere objektfunksjonen $L(θ,D)$, kjent som loss-function/loss-funksjonen i dyp læring. Ved en perfekt prediksjon vil vi ha $L(θ,D)=0$, og $f_θ(x) = y$. For å oppnå dette bruker vi klassen cross-entropy. Her finner vi et mål på hvor like to sannsynlighetsfordelinger er, og gjør at vi kan sammenligne $onehot(y)$ med en konstruert $Ŷ$ som består av sannsynlighetsfordelingene til inputen vår. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.3\n",
    "Anta at vi bruker cross-entropy som objektfunksjon, at $m = 5 \\text{ og } y = [4, 3, 2, 1]$.\n",
    "Hvilke diskret sannsynlighetsfordeling $Ŷ$ ville gitt en objektfunksjon $L(θ, D) = 0$?\n",
    "Hva ville $ŷ$ vært i dette tilfellet?\n",
    "\n",
    "Dersom sannsynlighetsfordelingen $Ŷ = F_{\\text{θ}}(x)$ er identisk med $onehot(y)$, der $onhot(y)$ tillsvarer \"sannsynlighetsfordelingen\" fra vårt gitte datasett som kun består av nuller og entall. Får vi $L(θ, D) = 0$. Dette vil også\n",
    "bety at $ argmax_{col}(Ŷ) = ŷ = y $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.4 \n",
    "Gitt \n",
    "$\n",
    "d, m, n_{\\text{max}}, k, p \\text{ og } L\n",
    "$ .\n",
    " Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall w ∈ R vi må bestemme ved optimering. En matrise $ W \\in \\mathbb{R}^{m \\times n} $ består av $m \\cdot n$ tall, eller enkeltparametre.\n",
    "\n",
    "Vi ser på dimensjonene av alle parameterene til $θ$, bruken av lag gjør også at vi må skalere noen av parameterene med L. \n",
    "\n",
    "$\n",
    "L\\left(2(p \\cdot d) + 4(k \\cdot d)\\right) + 2(d \\cdot m) + (d \\cdot n_{\\text{max}})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitt:\n",
    "\n",
    "* $ n = n_{\\text{max}} = 1 $\n",
    "* $ m = d = k = p = 2 $\n",
    "* $ L = 1 $\n",
    "* $ W_{O} = W_{V} = W_{1} = W_{2} = W_{U} $\n",
    "* $ \\rho(x) = \\text{Relu}(x) = \\max(x,0) $\n",
    "* $ W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} $\n",
    "* $ W_{P} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $\n",
    "* Input: $x = [1]$\\\n",
    "* Ønsket output: $\\hat{z} = [1]$\n",
    "\n",
    "Vis at vi må ha $\\alpha > 1$. Benytter videre ligning **4-10**:\n",
    "\n",
    "**(4)** \\\n",
    "$X = onehot(x) = \n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**(5)** \\\n",
    "$z_0 = W_EX + [W_P]_{0:1} = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0+1\\\\\n",
    "\\alpha+0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "**(6)** \\\n",
    "$z_{l+1/2} = f_l^A(z_l) = z_l + W_O^TW_Vz_lA(z_l)$\\\n",
    "$z_{0+1/2} = f_l^A(z_0) = z_0 + W_O^TW_Vz_0A(z_0)$\\\n",
    "\n",
    "$W_O^T=W_V = I_{2\\times2} => W_0^TW_V = I_{2\\times2}$\n",
    "\n",
    "$A(z_0) = softmax_{col}(z_0^TW_Q^TW_kz_0+D)$\n",
    "\n",
    "$z_0^TW_Q^TW_kz_0 = \n",
    "\\begin{bmatrix}\n",
    "1 & \\alpha\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}\n",
    "= [1^2 + \\alpha^2]$\n",
    "\n",
    "Vi har derfor, $A(z_0) = [1^2 + \\alpha^2]$, slik at:\n",
    "\n",
    "$z_{0+1/2} = \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} [1^2 + \\alpha^2] = \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 + \\alpha^2 \\\\ \\alpha(1+\\alpha^2) \\end{bmatrix} = \\begin{bmatrix} 1 + 1 + \\alpha^2 \\\\ \\alpha+\\alpha(1+\\alpha^2) \\end{bmatrix}$\n",
    "\n",
    "**(7)** \\\n",
    "$z_{l+1} = f_l^L(z_{l+1/2})=z_{l+1/2}+W_2^T\\rho(W_1 z_{l+1/2})$\n",
    "$W_2^T = I_2^2$\n",
    "\n",
    "Aktiveringsfunksjonen $\\rho(x)=Relu(x)=max(0,x)$\n",
    "\n",
    "$\\rho(w_1z_{0+1/2}) = max(0,w_1z_{0+1/2}) = \\begin{bmatrix} \\rho_1 \\\\ \\rho_2 \\end{bmatrix} $\n",
    "\n",
    "$z_{l+1} = \\begin{bmatrix} 1 + 1 + \\alpha^2 + \\rho_1\\\\ \\alpha+\\alpha(1+\\alpha^2)+\\rho_2 \\end{bmatrix} = \\begin{bmatrix} 2(1+1(1 + \\alpha^2)) \\\\ 2(\\alpha+\\alpha(1+\\alpha^2))\\end{bmatrix} $\n",
    "\n",
    "**(8)** \\\n",
    "$Z = softmax_{col}(W_U^Tz_l) = softmax_{col}(\\begin{bmatrix} 2(1+1(1 + \\alpha^2)) \\\\ 2(\\alpha+\\alpha(1+\\alpha^2))\\end{bmatrix}) = \\begin{bmatrix} soft_1\\\\ soft_2 \\end{bmatrix}$\n",
    "\n",
    "**(9)** \\\n",
    "$\\hat{z} = argmax_{col}(Z) = argmax_{col}(\\begin{bmatrix} soft_1\\\\ soft_2 \\end{bmatrix})$\n",
    "$\\hat{z} = [1]$ dersom $soft_1<soft_2$. Dette stememr for $\\alpha>1$\n",
    "\n",
    "Vi har altså nå vist at for å få $\\hat{z} = [1]$ som output, når vi har inputet $x = [1]$, er vi nødt til å ha at $\\alpha$ er større enn $1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2.1\n",
    "\n",
    "NeuralNetwork bruker arv, eller inheritence, når det itereres gjennom lagene. \n",
    "Inheritance-konseptet benytter objektorientert programmering hvor funksjonalitet og egenskaper fra en overordnet klasse kan deles til dens underklasser. Selv om NeuralNetwork ikke direkte etablerer underklasser med arv, organiserer og behandler den de forskjellige typer lagene innenfor det nevrale nettverket på en måte som gir det samme utbyttet som arv innenfor underklasser. NeuralNetwork slipper å gjenta kode for operasjoner som gjentas på flere lag, og er satt opp i moduler som gjør det enklere å lese. isinstance()-funksjonen bidrar i stor grad til dette, da den sørger for at layers av en viss type gjennomgår gradient descent på en bestemt måte. step_gd er også definert forskjellig for hver av klassene som arver fra Layer-klassen, som benytter arv direkte til sine underklasser. Disse elementene ved koden sørger for at NeuralNetwork har en form for konseptuell arv gjennom lag basert på deres felles funksjonalitet selv om den ikke direkte nedarver funksjonalitet til underklasser. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
