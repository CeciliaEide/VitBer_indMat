{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosjekt 2: Transformermodellen for prediksjon av sekvenser\n",
    "Av: Ingrid Løvold, Oskar Farbrot og Cecilia Eide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.1\n",
    "Gi et eksempel (som i likning (10)) på hvordan et datasett {x, y} ville sett ut for\n",
    "å trene en transformermodell for å predikere et heltall d gitt d = a · b + c der a, c\n",
    "er tosifrede heltall, mens b er et ettsifret heltall, altså 9 ≥ b ∈ Z\n",
    "\n",
    "Lengden av d vil kunne være på max 3 siffer (99*9 + 9 = 900) //c er også tosifret? 99*9 + 99 = 990\n",
    "\n",
    "x = [a0, . . . , ar−1, b0, . . . , br−2, c0, . . . , cr−1]   ; der ai, bi, ci ∈ {0, . . . , 9} //inkludere d0-(dr-1) (ref 3.1.1)?\n",
    "\n",
    "y = [d0, . . . , dr]\n",
    "\n",
    "\n",
    "La r = ??, a = 86, b = 4, c = 28, //r = 2?\n",
    "da har vi x = [8, 6, 4, 2, 8, 3, 7], y = [3, 7, 2]\n",
    "\n",
    "Modellen vil gi oss z = [ˆz0, . . . , zˆ7] = fθ([8, 6, 4, 2, 8, 3, 7])\n",
    "vi ønsker å finne θ slik at ˆy = [ˆz5, zˆ6, zˆ7] = y = [3, 7, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.2\n",
    "Når optimeringen er ferdig, hvordan kan vi bruke modellen fθ til å predikere d gitt\n",
    "a, b, c? Vis dette med et eksempel, på samme måte som i likning (11).\n",
    "\n",
    "--rekkefølgen på ^y flippes!\n",
    "\n",
    "x0 = [8,6,4,2,8]                [^z0,^z1,^z2,^z3,^z4,^z5] = fθ(x0)\n",
    "x1 = [8,6,4,2,8,^z]             [^z0,^z1,^z2,^z3,^z4,^z5,^z6] = fθ(x1)\n",
    "x2 = [8,6,4,2,8,^z,^z]          [^z0,^z1,^z2,^z3,^z4,^z5,^z6,^z7] = fθ(x2)\n",
    "x3 = [8,6,4,2,8,^z,^z,^z]       [^z0,^z1,^z2,^z3,^z4,^z5,^z6,^z7,^z8] = fθ(x3)\n",
    "\n",
    "sammenligner ^y med d for å se om modellen fungerer\n",
    "\n",
    "\n",
    "fθ har som mål å minimere objektfunksjonen L(θ,D), kjent som loss-function/loss-funksjonen i dyp læring. Ved en perfekt prediksjon vil vi ha L(θ,D)=0, og fθ(x) = y. For å oppnå dette bruker vi funksjonen cross-entropy som vår objektfunksjonen. Funksjonen gir et mål på hvor like to sannsynlighetsfordelinger er, og gjør at vi kan sammenligne onehot(y) med en konstruert Y^ som består av sannsynlighetsfordelingene til inputen vår med like mange kolonner som lengden av y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.3\n",
    "Anta at vi bruker cross-entropy som objektfunksjon, at $m = 5 \\text{ og } y = [4, 3, 2, 1]$.\n",
    "Hvilke diskret sannsynlighetsfordeling $Ŷ$ ville gitt en objektfunksjon $L(θ, D) = 0$?\n",
    "Hva ville $ŷ$ vært i dette tilfellet?\n",
    "\n",
    "Dersom $Ŷ = F_{\\text{θ}}(x)$ er identisk med $onehot(y)$ får vi $L(θ, D) = 0$. Dette vil også\n",
    "bety at $ argmax_{col}(Ŷ) = ŷ = y $.\n",
    "\n",
    "$m = 5$, altså kan treningsdataen bestå av tall mellom $0 \\text{ og } 4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.4 \n",
    "Gitt \n",
    "$\n",
    "d, m, n_{\\text{max}}, k, p \\text{ og } L\n",
    "$ .\n",
    " Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall w ∈ R vi må bestemme ved optimering. En matrise $ W \\in \\mathbb{R}^{m \\times n} $ består av $m \\cdot n$ tall, eller enkeltparametre.\n",
    "\n",
    "Vi ser på dimensjonene av alle parameterene til $θ$, bruken av lag gjør også at vi må skalere noen av parameterene med L. \n",
    "\n",
    "$\n",
    "L\\left(2(p \\cdot d) + 4(k \\cdot d)\\right) + 2(d \\cdot m) + (d \\cdot n_{\\text{max}})\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gitt:\n",
    "\n",
    "- $ n = n_{\\text{max}} = 1 $\n",
    "- $ m = d = k = p = 2 $\n",
    "- $ L = 1 $\n",
    "- $ W_{O} = W_{V} = W_{1} = W_{2} = W_{U} $\n",
    "- $ \\rho(x) = \\text{Relu}(x) = \\max(x,0) $\n",
    "- $ W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix} $\n",
    "- $ W_{P} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $\n",
    "\n",
    "Input:         $x = [1]$\\\n",
    "Ønsket output: $\\hat{z} = [1]$\n",
    "\n",
    "Vis at vi må ha $\\alpha > 1$. Benytter videre ligning 4-10:\n",
    "\n",
    "(4) $X = onehot(x) = \n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "(5) $z_0 = W_EX + [W_P]_{0:1}$\\\n",
    "$z_0 = \\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & \\alpha \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0+1\\\\\n",
    "\\alpha+0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "(6) $z_{l+1/2} = f_l^A(z_l) = z_l + W_O^TW_Vz_lA(z_l)$\\\n",
    "$z_{0+1/2} = f_l^A(z_0) = z_0 + W_O^TW_Vz_0A(z_0)$\\\n",
    "\n",
    "* $W_O^T=W_V = I_{2\\times2} => W_0^TW_V = I_{2\\times2}$\n",
    "* $A(z_0) = softmax_{col}(z_0^TW_Q^TW_kz_0+D)$\n",
    "* $z_0^TW_Q^TW_kz_0 = \n",
    "\\begin{bmatrix}\n",
    "1 & \\alpha\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}\n",
    "= [1^2 + \\alpha^2]$\n",
    "\n",
    "Vi har derfor, $A(z_0) = [1^2 + \\alpha^2]$, slik at:\n",
    "\n",
    "$z_{0+1/2} = \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} [1^2 + \\alpha^2] = \\begin{bmatrix} 1 \\\\ \\alpha \\end{bmatrix} + \\begin{bmatrix} 1 + \\alpha^2 \\\\ \\alpha(1+\\alpha^2) \\end{bmatrix} = \\begin{bmatrix} 1 + 1 + \\alpha^2 \\\\ \\alpha+\\alpha(1+\\alpha^2) \\end{bmatrix}$\n",
    "\n",
    "(7) $z_{l+1} = f_l^L(z_{l+1/2})=z_{l+1/2}+W_2^T\\rho(W_1 z_{l+1/2})$\n",
    "* $W_2^T = I_2^2$\n",
    "* aktiveringsfunksjonen $\\rho(x)=Relu(x)=max(0,x)$\\\n",
    "$\\rho(w_1z_{0+1/2}) = max(0,w_1z_{0+1/2}) = \\begin{bmatrix} \\rho_1 \\\\ \\rho_2 \\end{bmatrix} $\\\n",
    "$z_{l+1} = \\begin{bmatrix} 1 + 1 + \\alpha^2 + \\rho_1\\\\ \\alpha+\\alpha(1+\\alpha^2)+\\rho_2 \\end{bmatrix} = \\begin{bmatrix} 2(1+1(1 + \\alpha^2)) \\\\ 2(\\alpha+\\alpha(1+\\alpha^2))\\end{bmatrix} $\n",
    "\n",
    "(8) $Z = softmax_{col}(W_U^Tz_l) = softmax_{col}(\\begin{bmatrix} 2(1+1(1 + \\alpha^2)) \\\\ 2(\\alpha+\\alpha(1+\\alpha^2))\\end{bmatrix}) = \\begin{bmatrix} soft_1\\\\ soft_2 \\end{bmatrix}$\n",
    "\n",
    "(9) $\\hat{z} = argmax_{col}(Z) = argmax_{col}(\\begin{bmatrix} soft_1\\\\ soft_2 \\end{bmatrix})$\n",
    "\n",
    "\n",
    "$\\hat{z} = [1]$ dersom $soft_1<soft_2$. Dette stememr for $\\alpha>1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2.1\n",
    "\n",
    "NeuralNetwork bruker arv, eller inheritence, gjennom måten det itereres gjennom lagene. \n",
    "Inheritance-konseptet relaterer til objektorientert programmering hvor funksjonalitet og egenskaper fra en overordnet klasse kan deles til dens underklasser. Selv om NeuralNetwork ikke direkte etablerer underklasser med arv, organiserer og behandler den de forskjellige typer lagene innenfor det nevrale nettverket på en måte som gir det samme utbyttet som arv innenfor underklasser. NeuralNetwork slipper gjenta kode for operasjoner som gjentas på flere lag, og er satt opp i moduler som gjør det enklere å lese. isinstance()-funksjonen bidrar i stor grad til dette, da den sørger for at layers av en viss type gjennomgår gradient descent på en bestemt måte. step_gd er også definert forskjellig for hver av klassene som arver fra Layer-klassen, som benytter arv direkte til sine underklasser. Disse elementene ved koden sørger for at NeuralNetwork har en form for konseptuell arv gjennom underkategorisering av lag basert på deres felles funksjonalitet selv om den ikke direkte nedarver funksjonalitet til underklasser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har i filene layers.py og neural_network definert nødvendige klasser med tilhørende medlemsfunksjoner til å trene en transformermodell til å predikere neste tall i en sekvens. Vi skal gjøre dette og begynner med å trene en modell til å kunne addere to heltall. Parameterne skal da oppdateres slik at loss funksjonen blir mindre og mindre. Vi plotter gjennomsnittet av lossfunksjonen for ... som funksjon av antall iterasjoner og ser at dette blir tilfelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_algorithm import TrainingAlgorithmAdding, prosentSortetRight\n",
    "\n",
    "nn_adding, mean_losses_addding = TrainingAlgorithmAdding()\n",
    "\n",
    "n = np.np.arange(1, n+1)\n",
    "\n",
    "plt.xlabel('Antall iterasjoner')\n",
    "plt.ylabel('Gjennomsnittlig verdi av L')\n",
    "\n",
    "plt.plot(n, mean_losses_adding)\n",
    "\n",
    "prosent = prosentSortetRight(nn_adding)\n",
    "print(f'Prosentandel av input som ble riktig med nåværende modell: {prosent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
