{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  1.28252041635867 \n",
      "Iterasjon  1  L =  1.4916173022268 \n",
      "Iterasjon  2  L =  3.416255720316021 \n",
      "Iterasjon  3  L =  8.756217704906078 \n",
      "Iterasjon  4  L =  14.150802464454898 \n",
      "Iterasjon  5  L =  19.272806374266644 \n",
      "Iterasjon  6  L =  24.342993973861663 \n",
      "Iterasjon  7  L =  26.94942264411474 \n",
      "Iterasjon  8  L =  30.835714923978777 \n",
      "Iterasjon  9  L =  34.8474455232409 \n",
      "Iterasjon  10  L =  37.73075049381762 \n",
      "Iterasjon  11  L =  39.96222824842582 \n",
      "Iterasjon  12  L =  41.33409130681485 \n",
      "Iterasjon  13  L =  44.1149092605512 \n",
      "Iterasjon  14  L =  46.793438067280306 \n",
      "Iterasjon  15  L =  49.959660512251425 \n",
      "Iterasjon  16  L =  54.40757274498327 \n",
      "Iterasjon  17  L =  60.33007641874101 \n",
      "Iterasjon  18  L =  64.7215212747245 \n",
      "Iterasjon  19  L =  74.57564983255838 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bruker\\Documents\\04_STUDIE\\VitBer\\TMA4320-IndMat\\VitBer_indMat\\prosjekt_2_utlevert_kode\\layers.py:168: RuntimeWarning: divide by zero encountered in log\n",
      "  q = -np.log(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  20  L =  inf \n",
      "Iterasjon  21  L =  inf \n",
      "Iterasjon  22  L =  inf \n",
      "Iterasjon  23  L =  inf \n",
      "Iterasjon  24  L =  inf \n",
      "Iterasjon  25  L =  inf \n",
      "Iterasjon  26  L =  inf \n",
      "Iterasjon  27  L =  inf \n",
      "Iterasjon  28  L =  inf \n",
      "Iterasjon  29  L =  inf \n",
      "Iterasjon  30  L =  inf \n",
      "Iterasjon  31  L =  inf \n",
      "Iterasjon  32  L =  inf \n",
      "Iterasjon  33  L =  inf \n",
      "Iterasjon  34  L =  inf \n",
      "Iterasjon  35  L =  inf \n",
      "Iterasjon  36  L =  inf \n",
      "Iterasjon  37  L =  inf \n",
      "Iterasjon  38  L =  inf \n",
      "Iterasjon  39  L =  inf \n",
      "Iterasjon  40  L =  inf \n",
      "Iterasjon  41  L =  inf \n",
      "Iterasjon  42  L =  inf \n",
      "Iterasjon  43  L =  inf \n",
      "Iterasjon  44  L =  inf \n",
      "Iterasjon  45  L =  inf \n",
      "Iterasjon  46  L =  inf \n",
      "Iterasjon  47  L =  inf \n",
      "Iterasjon  48  L =  inf \n",
      "Iterasjon  49  L =  inf \n",
      "Iterasjon  50  L =  inf \n",
      "Iterasjon  51  L =  inf \n",
      "Iterasjon  52  L =  inf \n",
      "Iterasjon  53  L =  inf \n",
      "Iterasjon  54  L =  inf \n",
      "Iterasjon  55  L =  inf \n",
      "Iterasjon  56  L =  inf \n",
      "Iterasjon  57  L =  inf \n",
      "Iterasjon  58  L =  inf \n",
      "Iterasjon  59  L =  inf \n",
      "Iterasjon  60  L =  inf \n",
      "Iterasjon  61  L =  inf \n",
      "Iterasjon  62  L =  inf \n",
      "Iterasjon  63  L =  inf \n",
      "Iterasjon  64  L =  inf \n",
      "Iterasjon  65  L =  inf \n",
      "Iterasjon  66  L =  inf \n",
      "Iterasjon  67  L =  inf \n",
      "Iterasjon  68  L =  inf \n",
      "Iterasjon  69  L =  inf \n",
      "Iterasjon  70  L =  inf \n",
      "Iterasjon  71  L =  inf \n",
      "Iterasjon  72  L =  inf \n",
      "Iterasjon  73  L =  inf \n",
      "Iterasjon  74  L =  inf \n",
      "Iterasjon  75  L =  inf \n",
      "Iterasjon  76  L =  inf \n",
      "Iterasjon  77  L =  inf \n",
      "Iterasjon  78  L =  inf \n",
      "Iterasjon  79  L =  inf \n",
      "Iterasjon  80  L =  inf \n",
      "Iterasjon  81  L =  inf \n",
      "Iterasjon  82  L =  inf \n",
      "Iterasjon  83  L =  inf \n",
      "Iterasjon  84  L =  inf \n",
      "Iterasjon  85  L =  inf \n",
      "Iterasjon  86  L =  inf \n",
      "Iterasjon  87  L =  inf \n",
      "Iterasjon  88  L =  inf \n",
      "Iterasjon  89  L =  inf \n",
      "Iterasjon  90  L =  inf \n",
      "Iterasjon  91  L =  inf \n",
      "Iterasjon  92  L =  inf \n",
      "Iterasjon  93  L =  inf \n",
      "Iterasjon  94  L =  inf \n",
      "Iterasjon  95  L =  inf \n",
      "Iterasjon  96  L =  inf \n",
      "Iterasjon  97  L =  inf \n",
      "Iterasjon  98  L =  inf \n",
      "Iterasjon  99  L =  inf \n",
      "Iterasjon  0  L =  1.1713284426091397 \n",
      "Iterasjon  1  L =  1.4087205933426032 \n",
      "Iterasjon  2  L =  1.501647241794912 \n",
      "Iterasjon  3  L =  1.700751809809938 \n",
      "Iterasjon  4  L =  1.806926600243968 \n",
      "Iterasjon  5  L =  1.8919505719795702 \n",
      "Iterasjon  6  L =  1.9144183921469093 \n",
      "Iterasjon  7  L =  1.9386782582754698 \n",
      "Iterasjon  8  L =  1.9217735761416204 \n",
      "Iterasjon  9  L =  1.9062663670761726 \n",
      "Iterasjon  10  L =  1.867936309197475 \n",
      "Iterasjon  11  L =  1.8043840925458408 \n",
      "Iterasjon  12  L =  1.7600200341414183 \n",
      "Iterasjon  13  L =  1.7385034240178503 \n",
      "Iterasjon  14  L =  1.6697082745121148 \n",
      "Iterasjon  15  L =  1.7571905137645256 \n",
      "Iterasjon  16  L =  2.0730900064949034 \n",
      "Iterasjon  17  L =  2.096117042006395 \n",
      "Iterasjon  18  L =  1.980752043364666 \n",
      "Iterasjon  19  L =  1.8552625977930102 \n",
      "Iterasjon  20  L =  2.225653963252346 \n",
      "Iterasjon  21  L =  2.0299703965353233 \n",
      "Iterasjon  22  L =  1.6939512556918532 \n",
      "Iterasjon  23  L =  1.538134016000672 \n",
      "Iterasjon  24  L =  1.6015468092697382 \n",
      "Iterasjon  25  L =  1.842581999671859 \n",
      "Iterasjon  26  L =  2.079178701732699 \n",
      "Iterasjon  27  L =  1.886668191469712 \n",
      "Iterasjon  28  L =  1.8128235629833802 \n",
      "Iterasjon  29  L =  1.7648371559010587 \n",
      "Iterasjon  30  L =  1.8074652082122316 \n",
      "Iterasjon  31  L =  1.7210965147121282 \n",
      "Iterasjon  32  L =  1.605532591229391 \n",
      "Iterasjon  33  L =  1.7167984429774705 \n",
      "Iterasjon  34  L =  1.6897925042758029 \n",
      "Iterasjon  35  L =  1.6369330677377547 \n",
      "Iterasjon  36  L =  2.1849006997136007 \n",
      "Iterasjon  37  L =  2.127984462563439 \n",
      "Iterasjon  38  L =  1.6633638960644703 \n",
      "Iterasjon  39  L =  1.6135407522026426 \n",
      "Iterasjon  40  L =  1.8542987423016506 \n",
      "Iterasjon  41  L =  2.042862916469573 \n",
      "Iterasjon  42  L =  2.1105330433909844 \n",
      "Iterasjon  43  L =  2.177645031357879 \n",
      "Iterasjon  44  L =  1.8694414778995316 \n",
      "Iterasjon  45  L =  1.637001350965674 \n",
      "Iterasjon  46  L =  1.5512180657550814 \n",
      "Iterasjon  47  L =  1.6009391408819187 \n",
      "Iterasjon  48  L =  1.563150432829375 \n",
      "Iterasjon  49  L =  1.5074244861066208 \n",
      "Iterasjon  50  L =  1.5188188583442996 \n",
      "Iterasjon  51  L =  1.5104821047907806 \n",
      "Iterasjon  52  L =  1.5210607299424044 \n",
      "Iterasjon  53  L =  1.5241846004171813 \n",
      "Iterasjon  54  L =  1.5272569656702704 \n",
      "Iterasjon  55  L =  1.531093078601594 \n",
      "Iterasjon  56  L =  1.5440551851892699 \n",
      "Iterasjon  57  L =  1.5467072210602848 \n",
      "Iterasjon  58  L =  1.5327904363994005 \n",
      "Iterasjon  59  L =  1.5397676249232213 \n",
      "Iterasjon  60  L =  1.5283110984419668 \n",
      "Iterasjon  61  L =  1.527988960193298 \n",
      "Iterasjon  62  L =  1.5397464642897842 \n",
      "Iterasjon  63  L =  1.5086983433883607 \n",
      "Iterasjon  64  L =  1.53434031983695 \n",
      "Iterasjon  65  L =  1.5176120313273813 \n",
      "Iterasjon  66  L =  1.5302941218109605 \n",
      "Iterasjon  67  L =  1.5214953234607946 \n",
      "Iterasjon  68  L =  1.5320136814395204 \n",
      "Iterasjon  69  L =  1.5316002161844366 \n",
      "Iterasjon  70  L =  1.5288024716267405 \n",
      "Iterasjon  71  L =  1.5494224454397725 \n",
      "Iterasjon  72  L =  1.5609578425904367 \n",
      "Iterasjon  73  L =  1.5275581751558946 \n",
      "Iterasjon  74  L =  1.5421860589632437 \n",
      "Iterasjon  75  L =  1.5347408359045407 \n",
      "Iterasjon  76  L =  1.5353066277858445 \n",
      "Iterasjon  77  L =  1.572574618287073 \n",
      "Iterasjon  78  L =  1.5639552869497284 \n",
      "Iterasjon  79  L =  1.566264496126427 \n",
      "Iterasjon  80  L =  1.578905487549736 \n",
      "Iterasjon  81  L =  1.5988037161447806 \n",
      "Iterasjon  82  L =  1.6015722373040604 \n",
      "Iterasjon  83  L =  1.505818778250973 \n",
      "Iterasjon  84  L =  1.551793882502942 \n",
      "Iterasjon  85  L =  1.5480857948841522 \n",
      "Iterasjon  86  L =  1.534558825423153 \n",
      "Iterasjon  87  L =  1.5646555091006857 \n",
      "Iterasjon  88  L =  1.5197621983729575 \n",
      "Iterasjon  89  L =  1.5576319957099904 \n",
      "Iterasjon  90  L =  1.5106648209230855 \n",
      "Iterasjon  91  L =  1.5554076248804243 \n",
      "Iterasjon  92  L =  1.494091061236187 \n",
      "Iterasjon  93  L =  1.5539184435673765 \n",
      "Iterasjon  94  L =  1.51085288242925 \n",
      "Iterasjon  95  L =  1.5418935413340842 \n",
      "Iterasjon  96  L =  1.5131210315468793 \n",
      "Iterasjon  97  L =  1.4970790965536147 \n",
      "Iterasjon  98  L =  1.5517066796810346 \n",
      "Iterasjon  99  L =  1.5276682435748075 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training_algorithm import TrainingAlgorithm\n",
    "\n",
    "nn_adding_0, mean_losses_adding_0 = TrainingAlgorithm(1) #addisjonsproblem\n",
    "\n",
    "nn_adding_1, mean_losses_adding_1 = TrainingAlgorithm(0) #sorteringsproblem\n",
    "\n",
    "n = np.arange(1, len(mean_losses_adding_1)+1)\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(n, mean_losses_adding_0, label='Addisjonsproblem')\n",
    "plt.plot(n, mean_losses_adding_1, label='sorteringsproblem')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under trening av algoritmen opplever vi to større feil ved minimering av Loss-funksjonen:\\\n",
    "**Feil 1:** Både step_Adam og step_gd gir en økning i verdien til Loss-funksjonen. Step_Adam varierer før den divergerer, til forskjell fra step_gd som er helt lineær og svakt økende.\\\n",
    "**Feil 2:** Bruk av step_Adam gir \"RuntimeWarning: divide by zero encountered in log q = -np.log(p)\" for attention forward. \n",
    "\n",
    "Under debuggingen  har vi utforsket følgende hypoteser: \n",
    "1) Matriseelementer i p i attention forward er lik null, og np.log(p) gir feilmenldinger da logaritmen til 0 ikke eksisterer. Det er flere grunner til at p kan bli lik null, blant dem:\n",
    "    - self.Y = onehot(y,m) blir null: dersom onehot ikke fungerer som den skal og og ikke leverer verdier lik 1 vil den kunne gi en null-matrise for self.Y = onehot(y,m). Vi sjekket derfor max-verdien i hver kolonne til onehot-matrisen og fikk at samtlige hadde en verdi 1. Onehot fungerer derfor tilsynelatende som den skal. \n",
    "    - self.Y_hat = Z[:,:,-r:] blir null: dersom Z = nn.forward(X) ikke fungerer slik den skal vil den kunne gi ut en Z som blir enten null eller veldig liten. Til å teste dette printet vi retur-verdien til hvert lag i forward. Vi oppdager at flere verdier i softmax divergerer mot null men tester samtidig kollonnesummene og ser at disse er lik 1 (eller tilnærmet lik +-10**-8). Siden Softmax er en sannsynlighetsfordeling kan det stemme at flere av elementene har sannsynligheter som er nærmest lik null, og siden kollonnesummene stemmer finner vi ingen åpenbare kilder til feil for Z.\n",
    "2) Dimensjonsfeil \n",
    "    - dersom slizingen av Z ikke er gjort riktig og gir feil self.Y_hat = Z kan dette være en kilde til feil. Her sjekket vi derfor at både Y_hat og Y har like dimensjoner (\"legge inn dim her\")\n",
    "    - feil bruk av einsum vil kunne gi feil dimensjoner som kan gå utover f.eks np.sum()-funksjonene som behandler sannsynlighetsfordelingene i forward pass eller på andre måter gjøre at informasjon går tapt i nettverket. Vi printet \"shapen\" til matrisene og forsikret oss om at de gir ut riktige dimensjoner. \n",
    "    - sjekket bruk av np.sum(arr,axis=)\n",
    "3) Feil i Adam (løser ikke problemet ved bruk av step_gd dvs. tilsynelatende ligger problemet et annet sted?)\n",
    "    - hvada?\n",
    "4) Feil i training_algorithm\n",
    "    - Testet for flere attention lag: Ved økning av lag gikk loss-funksjonen fortere mot inf, flere att. løser ikke problemet\n",
    "    - step_size har sammenheng med konvergenshastighen, og en endring i step_size vil påvirke hvor raskt eller sakte algoritmen konvergerer mot et optimalt punkt. I vårt tilfelle hvor funkjonen konvergerer vekk fra det optimale erfarte vi derfor at en mindre step_size ga funksjonen flere verdier før inf ble nådd. Selv om vi med en mindre step_size kunne se at Loss-funksjonen for enkelte områder ble mindre for step_adam, så vi likevel ikke noe tegn til at Loss-fungeren konvergerte til en lavere verdi. Vi vurderer derfor ikke læringsraten som en sannsynlig feilkilde. \n",
    "5) Feil i implimentering av algoritmer:\n",
    "    - Ut i fra feilen vår og konklusjoner fra punkt 1) kunne det virke som det kunne være noe feil i implimenteringen av algoritmer, vi fant, etter mye stirring, en feil i CrossEntropy.Backward, vi hadde \"dLdY = (1/self.n)*(np.multiply(padded_Y,self.Z+eps))\" mens det skulle være \"dLdY = -(1/self.n)*(padded_Y/(self.Z+eps))\", dette rettet dessvere ikke opp i alt, men loss-funksjonen konvergerer ikke mot inf lenger!\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
