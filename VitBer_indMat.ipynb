{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.1\n",
    "Gi et eksempel (som i likning (10)) på hvordan et datasett {x, y} ville sett ut for\n",
    "å trene en transformermodell for å predikere et heltall d gitt d = a · b + c der a, c\n",
    "er tosifrede heltall, mens b er et ettsifret heltall, altså 9 ≥ b ∈ Z\n",
    "\n",
    "Lengden av d vil kunne være på max 3 siffer (99*9 + 9 = 900) //c er også tosifret? 99*9 + 99 = 990\n",
    "\n",
    "x = [a0, . . . , ar−1, b0, . . . , br−2, c0, . . . , cr−1]   ; der ai, bi, ci ∈ {0, . . . , 9} //inkludere d0-(dr-1) (ref 3.1.1)?\n",
    "\n",
    "y = [d0, . . . , dr]\n",
    "\n",
    "\n",
    "La r = ??, a = 86, b = 4, c = 28, //r = 2?\n",
    "da har vi x = [8, 6, 4, 2, 8, 3, 7], y = [3, 7, 2]\n",
    "\n",
    "Modellen vil gi oss z = [ˆz0, . . . , zˆ7] = fθ([8, 6, 4, 2, 8, 3, 7])\n",
    "vi ønsker å finne θ slik at ˆy = [ˆz5, zˆ6, zˆ7] = y = [3, 7, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.2\n",
    "Når optimeringen er ferdig, hvordan kan vi bruke modellen fθ til å predikere d gitt\n",
    "a, b, c? Vis dette med et eksempel, på samme måte som i likning (11).\n",
    "\n",
    "--rekkefølgen på ^y flippes!\n",
    "\n",
    "x0 = [8,6,4,2,8]                [^z0,^z1,^z2,^z3,^z4,^z5] = fθ(x0)\n",
    "x1 = [8,6,4,2,8,^z]             [^z0,^z1,^z2,^z3,^z4,^z5,^z6] = fθ(x1)\n",
    "x2 = [8,6,4,2,8,^z,^z]          [^z0,^z1,^z2,^z3,^z4,^z5,^z6,^z7] = fθ(x2)\n",
    "x3 = [8,6,4,2,8,^z,^z,^z]       [^z0,^z1,^z2,^z3,^z4,^z5,^z6,^z7,^z8] = fθ(x3)\n",
    "\n",
    "sammenligner ^y med d for å se om modellen fungerer\n",
    "\n",
    "\n",
    "fθ har som mål å minimere objektfunksjonen L(θ,D), kjent som loss-function/loss-funksjonen i dyp læring. Ved en perfekt prediksjon vil vi ha L(θ,D)=0, og fθ(x) = y. For å oppnå dette bruker vi funksjonen cross-entropy som vår objektfunksjonen. Funksjonen gir et mål på hvor like to sannsynlighetsfordelinger er, og gjør at vi kan sammenligne onehot(y) med en konstruert Y^ som består av sannsynlighetsfordelingene til inputen vår med like mange kolonner som lengden av y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.3\n",
    "Anta at vi bruker cross-entropy som objektfunksjon, at m = 5 og y = [4, 3, 2, 1].\n",
    "Hvilke diskret sannsynlighetsfordeling Yˆ ville gitt en objektfunksjon L(θ, D) = 0?\n",
    "Hva ville ˆy vært i dette tilfellet?\n",
    "\n",
    "Dersom Yˆ = Fθ(x) er identisk med onehot(y) får vi L(θ, D) = 0. Dette vil også\n",
    "bety at argmaxcol(^Y ) = ˆy = y.\n",
    "\n",
    "m = 5, altså kan treningsdataen bestå av tall mellom 0 og 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.4 \n",
    "Gitt d, m, nmax, k, p og L. Hvor mange enkeltparametre har en transformermodell? Med enkeltparametre mener vi hvor mange tall w ∈ R vi må bestemme ved optimering. En matrise W ∈ R^m×n består av m · n tall eller enkeltparametre.\n",
    "\n",
    "ser på dimensjonene av alle parameterene til θ, merk også bruken av lag gjør at vi må skalere noen av parameterene med L\n",
    "\n",
    "L(2*(p*d) + 4*(k*d))  +  2*(d*m) + (d*n_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 1.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2.1\n",
    "\n",
    "NeuralNetwork bruker arv, eller inheritence, gjennom måten det itereres gjennom lagene. \n",
    "Inheritance-konseptet relaterer til objektorientert programmering hvor funksjonalitet og egenskaper fra en overordnet klasse kan deles til dens underklasser. Selv om NeuralNetwork ikke direkte etablerer underklasser med arv, organiserer og behandler den de forskjellige typer lagene innenfor det nevrale nettverket på en måte som gir det samme utbyttet som arv innenfor underklasser. NeuralNetwork slipper gjenta kode for operasjoner som gjentas på flere lag, og er satt opp i moduler som gjør det enklere å lese. isinstance()-funksjonen bidrar i stor grad til dette, da den sørger for at layers av en viss type gjennomgår gradient descent på en bestemt måte. step_gd er også definert forskjellig for hver av klassene som arver fra Layer-klassen, som benytter arv direkte til sine underklasser. Disse elementene ved koden sørger for at NeuralNetwork har en form for konseptuell arv gjennom underkategorisering av lag basert på deres felles funksjonalitet selv om den ikke direkte nedarver funksjonalitet til underklasser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
